# ReLU:
### <u>Name: Rectified Linear Unit</u>
ReLu ist eine weit verbreitete Aktivierungsfunktion, die die positiven Werte unverändert beibehält und negative Werte auf 0 setzt

![Funktion](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/1200px-Activation_rectified_linear.svg.png)

<br>
<br>
<br>

# sigmoid
### <u> Name: sigmoid </u>
Die Sigmoid-Funktion nimmt eine beliebige reelle Zahl und mappt sie auf den Bereich zwischen 0 und 1. Die Sigmoid-Funktion wird oft für binäre Klassifikationsprobleme verwendet, da sie eine Wahrscheinlichkeitsinterpretation ermöglicht.

![Funktion](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png)

<br>
<br>
<br>

# softmax
### <u> Name: Softmax </u>
Die Softmax-Funktion wird häufig in Multi-Klassen-Klassifikationsproblemen verwendet. Sie mappt die Eingabewerte auf eine Wahrscheinlichkeitsverteilung über verschiedene Klassen. Die Softmax-Funktion sorgt dafür, dass die Summe der Ausgabewerte 1 ergibt.

![Funktion](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1920px-Kernel_Machine.svg.png)

<br>
<br>
<br>

# TanH
### <u> Name: Hyperbolic Tangent </u>
Die TanH-Funktion ist eine Skalierung der Sigmoid-Funktion und mappt die Eingabewerte auf den Bereich zwischen -1 und 1. Sie wird oft für Klassifikationsprobleme verwendet.

![Funktion](https://upload.wikimedia.org/wikipedia/commons/7/76/Sinh_cosh_tanh.svg?download)

